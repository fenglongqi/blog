# 第九章 Web机器人

Web机器人是能够在无需人类干预的情况下自动进行一系列Web事务处理的软件程序。很多机器人会从一个Web站点逛到另一个Web站点，获取内容，跟踪超链，并对他们找到的数据进行处理。根据这些机器人自动探查Web站点的方式，人们为它们齐了一些格局特色的名字，比如"爬虫","蜘蛛"，“蠕虫”以及“机器人”等。

## 9.1 爬虫及爬行方式

Web爬虫是一种机器人，它们会递归地对各种信息性Web站点进行遍历，获取第一个Web页面，然后获取那个页面指向的所有Web页面，然后是那些页面指向的艘油Web页面，以此类推。

因特网搜索引擎使用爬虫在Web上游荡，并把他们碰到的文档全部拉回来。然后这些文档进行处理，形成一个可搜索的数据库，以便用户查找包含了特定单词的文档。网上有数万亿的Web页面需要查找和取回，这些搜索引擎蜘蛛必然是些最复杂的机器人。我们来进一步仔细地看看这些爬虫是怎样工作的。


### 9.1.1 从哪儿开始：根集

在爬虫放出去之前，需要给它一个起始点。爬虫开始访问的URL初始集合被称作根集时，应该从足够多不同的站点中选择URL，这样爬遍所有的连接才能最终到达大部分你感兴趣的Web页面。

通常，一个好的根集会包括一些大的流行Web站点、一个新穿件页面的列表和一个不经常被连接的无名页面列表。很多大规模的爬虫产品，比如因特网搜索引擎使用的爬虫，都为用户提供了向根集中提交新页面或无名页面的方式。这个根集会碎时间推移而增长，是所有新爬虫的种子列表。

### 9.1.2 链接的提取以及相对链接的标准化

爬虫在Web上移动式，会不停地对HTML页面进行解析。它要对所解析的每个页面上URL链接进行分析，并将这些链接添加到需要爬行的页面列表中去。随着爬虫的前进，当其发现需要探查的新连接时，这个列表常常会迅速地扩张。爬虫要通过简单的HTML解析，将这些链接提取出来，并将相对URL转换为绝对形式。


### 9.1.3 避免将环路的出现

机器人在Web上爬行时，要特别小心不要陷入循环，或环路之中。

机器人必须知道它们到过何处，以避免环路的出现。环路会造成机器人陷阱，这些陷阱会暂停或减缓机器人的爬行进程。

### 9.1.4 循环与复制

至少处于下列三个原因，环路对爬虫来说是有害的。

* 它们会使爬虫陷入可能会将其困住的循环之中。循环会使未经良好设计的爬虫不停地兜圈子，吧所有时间都耗费在不停地获取相同的页面上。
* 爬虫不断地获取相同的页面时，另一端的Web服务器也在遭受着打击。如果爬虫与服务器连接良好，它就会击垮Web站点，组织所有真实用户访问这个站点。这种拒绝服务器时可以作为法鲁诉讼理由的。
* 即使循环自身不是问题，爬虫也是在获取大量重复的页面。爬虫应用程序会被重复的内容所充斥，这样应用程序就会变得毫无用处。

### 9.1.5 面包屑留下的痕迹
但是。记录曾江到过那些地方并不总是一件容易的事。

如果要爬行世界范围内的一大块Web内容，就要做好访问数十亿URL的准备。由于URL的数量巨大，所以要是用复杂的数据节后以便快速判定那些URL是曾经访问过得。数据结构在访问速度和内存使用方面都应该是非常高效的。

数亿URL需要具备快速搜索结构，所以速度是很重要的。穷举搜索URL列表是根本不可能的。机器人至少要用到搜索树或散列表，以快速判定某个URL是否被访问过。

数亿URL还会占用大量的空间。如果平均每个URL有40个字符长，而且一个Web机器人要爬行5亿个URL，那么搜索数据结构只是存储这些URL就需要20GB或更多的存储空间。

这里列出了大规模Web爬虫对其访问过得地址进行管理时使用的一些有用的技术。

* **树和散列表** 复杂的机器人可能会搜索树或散列表来记录已访问的URL。这些事加速URL查找的软件数据结构。
* **有损的存在位图** 为了减少空间，一些大型爬虫会使用有损数据结构，不如存在位数组。用一个散列函数将每个URL都转换成定长的数字，这个数字在数组中有个相关的“存在位”。爬行过一个URL时，就将相应的“存在位”置位。如果存在位已经置位了，爬虫就认为已经爬行过那个URL了。
* **检查点** 一定要将一访问URL列表保存到硬盘上，以防机器人程序崩溃。
* **分类** 随着Web的扩展，在一套计算机上通过单个机器人来完成爬行就变得不太现实了。有些大型Web机器人会使用机器人“集群”，每个独立的计算机是一个机器人，以汇接方式工作。为每个机器人分配一个特定的URL“片”，尤其负责爬行。机器人个体之间可能需要相互通信，来回传送URL，以覆盖出故障的对等实体的爬行范围，或协调其工作。


### 9.1.6 别名与机器人环路

由于URL“别名”的存在，即使使用了正确的数据结构，有时也很难分辨出是以前访问过某个页面。如果两个URL看起来不一样，但实际指向的是同一资源，就称这两个URL互为“别名”。

下表列出了不同URL指向同一资源的几种简单方式。

![](https://i.imgur.com/uuJtlTa.png)


### 9.1.7 规范化URL

大多数Web机器人都试图通过URL“规范化”为标准格式来消除上面那些显而易见的别名。

1. 如果没有指定端口的话，就向主机名添加“:80”
2. 将所有转义字符%xx都转换成等价字符。
3. 删除#标签

经过这些步骤就可以消除上表a~c的别名问题。但如果不知道特定Web服务器的相关信息，机器人就没什么好办法来避免d~f的问题。

URL规范化可以消除一些基本的语法别名，但机器人还会遇到其他的、将URL转换为标准形式也无法消除的URL别名。

### 9.1.8 文件系统连接环路

文件系统中的符号连接会造成特定的潜在环路，因为它们会在目录层次深度有限的情况下，造成深度无线的家乡。符号连接环路通常是有服务器管理员的无心错误造成的。

![](./10.png)

在上图的b文件体统省，坑你会发生下列情况。

1. GET HTTP://www.foo.com/index.html <br>获取/idnex.html找到执行subdir/index.html的链接。
2. GET HTTP://www.foo.com/subdir/index.html <br>获取subdir/index.html,但得到的还是同一个index.html。
3. GET HTTP://www.foo.com/subdir/subdir/index.html <br>获取subdir/subdir/index.html
4. GET HTTP://www.foo.com/subdir/subdir/subdir/index.html <br>获取subdir/subdir/subdir/index.html


b中问题是subdir/是个指向“/”的环路，但由于URL看起来有所不同，所以机器人无法但从URL本省判断出文档是相同的。毫无戒备的机器人就有了陷入循环的危险。如果没有某种循环检测方式，这个环路就会继续下去，通常会持续到URL的长度超过机器人或服务器的限制为止。


### 9.1.9 动态虚拟Web空间

更常见的情况是，那些没有恶意的网管们可能会在无意中通过符号连接或动态内容构造出爬虫陷阱。比如。我们看一个基于CGI的日历程序，它会生成一个月历和一个纸箱些个月的连接。真正的用户是不会不停地请求下个月的链接的，但不了解其内容的动态性特性的机器人可能会不断向这些自愿发出无穷的请求。

### 9.1.10 避免循环和重复

没有什么简单明了的方式可以避免所有的环路。实际上，经过良好设计的机器人中药包含一组试探方式，以避免环路的出现。

总的来说，爬虫的自动化成都越高，越可能陷入麻烦之中。机器人的实现者需要做一些取舍——这些试探方式有助于避免问题的出现，但你可能会终止扫描呢些看起来可以的有效内容，因此这种方式也是“有损失”的。

在机器人会遇到的各种危险的Web中，有些技术的使用可以使机器人有更好的表现。

> 规范化URL
> > 将URL转换为标准形式以避免语法上的别名

>广度优先的爬行
>>每次爬虫都有大量潜在的URL要去爬行。以广度优先的方式调度URL去访问Web站点，就可以将环路的影响最小化。即使碰到了机器人陷阱，也可以在回到环路中获取的下一个页面之前，从其他Web站点中获取成百上千的页面。如果采用深度优先方式，一头扎到单个站点中去，就可能会跳入环路，永远无法访问其他站点。

>节流
>>限制一端时间内机器人可以从一个Web站点获取的页面数量。如果机器人跳进了一个环路，驶入不断地访问某个站点的别名，也可以通过节流来限制重复的页面总数和对服务器的访问总数。

>限制URL的大小
>>机器人可能会拒绝超出特定长度（通常是1KB）的URL。如果环路使URL的长度增加，长度限制就会最终终止这个环路。有些Web服务器在使用长URL时会失败，因此，被URL增长环路困住的机器人会使某些Web服务器崩溃。这回让网管错误地将机器人当成发起拒绝服务供给的攻击者。
>>
>要小心，这种技术肯定会让你错过一些内容。现在很多站点都会用URL来管理用户的状态。用URL长度来限制爬虫可能会带来些麻烦；但如果每当请求的URL达到某个特定长度时，都记录一次错误的话，就可以为yoghurt提供一种价差某特定站点上发生情况的方法。

>URL/站点黑名单
>>维护一个与机器人环路和陷阱相对应的一致站点及URL列表，然后避开他们。发现新问题时，就将其加入黑名单。
>>
>这就要求有人工进行干预。先现在很多大型爬虫产品都有某种心事的黑名单，用于避开某些存在固有问题或者有点恶意的站点。还可以用黑名单来避开那些对爬行大惊小怪的站点。

>模式检测
>>文件系统的符号连接和类似的错误配置所造成的华路会遵循某种模式；比如URL会随着组建的复制组件增加。有些机器人会将具有重复组件的URL当做潜在的环路，拒绝爬行带有多余两或三个重复组件的URL。
>>
>重复并不都是立即出现的。有些环路周期可能为2或其他间隔，比如“/subdir/images/subdir/images/subdir/images/...”。游侠机器人会查找具有几种不同周期的重复模式。

>内容指纹
>>一些更复杂的Web爬虫会使用指纹这种更直接的方式来检测重复。使用内容指纹的机器人会获取页面内容中的字节，并计算出一个校验和。这个校验和是页面内容的压缩表示形式。如果机器人获取了一个页面，而此页面的校验和它曾建见过，它就不会再去爬行这个页面的链接了。
>>
>必须对校验和函数进行选择，以求两个不同的页面拥有相同校验和的几率非常低。MD5这样的报文摘要函数就常被用于指纹计算。
>>
>有些Web服务器会在传输过程中对页面惊醒动态的修改，所以有时机器人会在校验和的计算中忽略Web页面内容中的某些部分，比如那些嵌入的链接。而且无论制定了什么页面内容的动态服务器端包含都可能会阻碍重复检测。

>人工监视
>>Web就是一片荒野。勇敢的机器人最终会陷入一个采用任何技术都无能为力的困境。实际所有产品级机器人时都要有诊断和日志功能，这样人类才能很方便地监视机器人的进展，如果返生了什么不寻常的事情就可以很快收到警告。在某些情况下，愤怒的王敏会给你发送一些无礼的邮件来提示你出了问题。

## 9.2 机器人的HTTP

机器人和所有其他HTTP客户端程序并没有什么区别。它们也要遵守HTTP规范中的规则。发出HTTP请求并将自己广播成HTTP/1.1客户端的机器人也要使用正确的HTTP请求首部。

很多机器人都试图只是先请求它们所查找内容所需的最小HTTP集。这会引发一些问题；但短期内这种行为不会发生什么改变。结果就是，很多机器人发出的都是HTTP/1.0请求，因为这个协议的要求很少。


### 9.2.1 识别请求首部

尽管机器人倾向于只支持最小的HTTP集，但大部分机器人确实实现并发送了一些识别首部——最值得一提的就是User-Agent首部。建议机器人实现者们发送一些基本的首部信息，以通知各站点机器人的能力、机器人的标识符，以及它是从何处起源的。

在追踪错误爬虫的所有者，以及向服务器提供机器人所能处理的内容类型是，这些信息都是很受用的。鼓励机器人实现者们使用的基本识别首部包括如下内容。

* User-Agent 将发起请求的机器人名字告知服务器
* From 提供机器人的用户/管理者的E-mail地址
* Accept 告知服务器可以发送那些媒体类型。这有助于确保机器人只接收它感兴趣的内容。
* Referer 提供包含了当前请求URL的文档的URL


### 9.2.2 虚拟主机

机器人实现者要支持Host首部。随着虚拟主机的流行，请求中不包含Host首部的话，可能会使机器人将错误的内容与一个特定的URL关联起来。因此，HTTP/1.1要求使用Host首部。

在默认情况下，大多数服务器都配置为提供一个特定的站点。因此，不包含Host首部的爬虫向提供两个站点的服务器发送请求时，会获取默认站点的内容。

### 9.2.3 条件请求

有些机器人实现了条件HTTP请求，它们会对时间戳或实体标签进行比较，看看它们最近获取的版本是否已将升级了。这与HTTP缓存查看以获取资源的本地副本是够有效的方法非常相似。


### 9.2.4 对响应的处理
很多机器人的兴趣主要在于用简单的GET方法来过去所请求的内容，所以，一般不会再处理相应的方式上花费太多时间。但是，使用了某些HTTP特性（比如条件请求）的机器人，以及那些想要更好地探索服务器，并与服务器进行交互的机器人则要能巩固对各种不同类型的HTTP响应进行处理。

> 状态码
> >
> 总之，机器人知道应该能够处理一些常见的，以及与其的状态码。

>实体
>>除了HTTP首部所嵌的信息之外，机器人也会在试题中查找信息。HTML标签，比如元标签http-equiv，就是内容编写者用于嵌入资源附加信息的一种方式。

### 9.2.5 User-Agent导向

Web管理者应该记住，会有很多的机器人来访问它们的站点，因此要做好接受机器人请求的准备。很多站点会为不同的用户代理进行内容优化，并尝试这浏览器类型进行检测，以确保能够支持各种站点特性。这样的话，当实际的HTTP客户端根本不是浏览器，而是机器人的时候，站点为机器人提供的就会是出错页面而不是页面内容了。

站点管理这应该设计一个处理机器人请求的策略。比如，它们可以为所有其他特性不太丰富的浏览器和机器人开发一些页面，而不是将其内容限定浏览器所支持的范围。

## 9.3 行为不当的机器人

不守规矩的机器人会造成很多严重问题。这里列出了一些机器人可能会烦的错误，及其恶劣行为所带来的后果。

>失控机器人
>>机器人发起HTTP请求的速度要比Web上的人类快得多，它们通常都运行在具有快速网络链路的告诉计算机上。如果机器人存在编程逻辑错误，或者陷入了环路之中，就可能会向Web服务器发出大量的负载——很可能会使服务器过载，并拒绝为任何其他人提供服务。所有的机器人编写者都必须特别小心地设计一些保护措施，以避免失控机器人带来的危害。

>失效的URL
>>有些机器人会访问URL列表。如果一个Web站点对其内容进行了大量的修改，机器人可能会对大量不存在的URL发起请求。这回激怒某些Web站点的管理员。

>很长的错误URL
>>由于环路和编程错误的存在，机器人可能会向Web站点请求一些很大的、无意义的URL。害人郭URL足够长的话，就会降低Web服务器的性能，使Web服务器的访问日志杂乱不堪，甚至会使一些比较脆弱的Web服务器崩溃。

>爱打听的机器人
>>有些机器人可能会得到一些指向私有数据的URL，这样，通过因特网搜索迎请和其他应用程序就可以很方便地访问这些数据了。有可能会对因袭的侵犯。
>>
>通常，发生这种情况是由于机器人所跟踪的、指向“私有内容”的超链接已经存在。偶尔也会因为机器人非常热衷于寻找某站点上的文档而出现这种情况，很可能就是在没有显式超链接的情况下去获取某个目录的内容造成的。
>>
>从Web上获取大量数据的机器人的实现者们应该清楚，他们的机器人很可能会在某些地方获得敏感的数据。一旦被指出，就应该有某种机制可以讲这些数据求其，这是非常重要的。

>动态网关访问
>>机器人并不总是知道它们访问的是什么内容，机器人可能会获取一个内容来自网关应用程序的URL。在这种情况下，获取的数据可能会有特殊的目的，计算机的开销可能很高。很多Web站点管理员并不喜欢那些请求网关文档的幼稚机器人。


## 9.4 拒绝机器人访问

人们提出了一项简单的资源约束技术，可以将机器人阻挡在不适合它的地方之外并为网站管理员提供了一种能够更好地控制机器人行为的机制。但通常只是根据存储访问控制信息的文件而将其称为robots.txt

### 9.4.1 拒绝机器人访问标准

机器人访问标准是一个临时标准，不同的厂商实现了这个标准的不同自己。

现在大多数气气人采用的都是标准v0.0或v1.0.

### 9.4.2 Web站点和robors.txt

如果一个Web站点有robots.txt文件，那么在访问这个Web站点上的任意URL之前，机器人都必须获取它并对其进行处理。

通常不能再Web站点上单独的子目录中安装“本地”robots.txt文件。网管要负责穿件一个聚合型robots.txt文件，用来描述Web站点上所有内容的拒绝访问规则。

>获取robots.txt
>>机器人会用HTTP的GET方法来获取robots.txt资源。如果有robots.txt文件的话，服务器会将其放在一个text/plain主体中返回。如果服务器以404 Not Found HTTP 状态码进行响应，机器人就可以人文这个服务器上没有机器人访问限制，他可以请求任意的文件。

>响应吗
>>如果服务器以一个成功状态（HTTP状态2XX）为响应，机器人就必须对内容进行解析，并使用排斥规则从那个站点上获取内容。
>>
>如果服务器响应说明资源不存在（HTTP状态码404），机器人就可以认为服务器没有激活任何排斥规则，对此站点的访问步骤robots.txt的限制。
>>
>如果服务器响应说明访问限制(HTTP状态码401或403)，机器人就应该认为对此站点的访问是完全受限的。
>>
>如果请求尝试的结果是临时故障（HTTP状态码503），机器人就应该推迟对此站点的访问，知道可以获取该资源为止。
>>
>如果服务器响应说明重定向（HTTP状态码3XX），气气人就应该跟着重定向，知道找到资源为止。

### 9.4.3 robots.txt文件的格式

robots.txt文件采用了非常简单的，面向行的语法。robots.txt文件中有三种类型的行：空行、逐世行和规则行。

robots.txt文件中的行为可以从逻辑上划分为“记录”。每条记录都为一组特定的机器人描述了一组排斥规则。通过这种方式，可以为不同的机器人使用不同的排斥规则。

每条记录中都包含了一组规则行，有一个空行或文件结束符终止。记录以一个或多个User-Agent行开始，说明哪些机器人会首次记录影响，后面跟着一些Disallow和Allow行，用来说明这些机器人可以访问哪些URL。

>User-Agent行
>>每个机器人记录都以一个或多个下列形式的User-Agent行开始：
>
	User-Agent:<robot-name>
	或
	User-Agent:*

>>在机器人HTTP GET请求的User-Agent首部发送机器人名
>>
>机器人处理robots.txt文件时，他说遵循的记录必须符合下列规则之一：
>>
>**第一个<robot-name>是机器人名的大小写无关的子字符串**
>>
>**第一个<robot-name>为"\*"**
>>
>如果机器人五发展到一起名字相匹配的User-Agent行，二部页无纺找到通配的User-Agent行，就是没有记录与之匹配，访问不受限。

>Disallow和Allow行
>>
>机器人必须将期望访问的URL按序与排斥记录中所有的Disallow和Allow规则进行匹配。使用找到的第一个匹配项
>>
>要使用Allow/Disallow行一亿个URL相匹配，规则路径就必须是URL路径大小写相关的前缀。例如，Disallow：/tmp就和下面所有的URL相匹配：
>
	http://www.joes-hardware.com/tmp
	http://www.joes-hardware.com/tmp/
	http://www.joes-hardware.com/tmp/pliers.html
	http://www.joes-hardware.com/tmp/stuff.txt


>Disallow/Allow前缀匹配
>>下面是Disallow/Allow前缀匹配的一些细节。
>
	Disallow和Allow规则要求大小写相关的前传匹配。这里的星号没有什么特设的含义，但空字符串可以起到通配符的效果。
>		
	在进行比较值钱，要将规则路径或URL路径中所有被转移的字符都反转为字节（撤了正斜杠%2F之外，它必须严格匹配）。
>	
	如果规则路径为空字符串，就与所有内容都匹配。

>>前缀匹配通常都能很好地工作，但有几种情况它的表达力不够强。如果你希望卢纶是用什么路径前缀，都不允许爬行一些特别的子目录，那robots.txt是无能为力的。

### 9.4.4 其他相关robots.txt的知识

解析robots.txt文件时还需遵循其他一些规则。

* 随着规范的发展，robots.txt文件中可能会包含除了User-Agent、Disallow和Allow置位的其他字段。机器人应该所有它不理解的字段都忽略掉。
* 为了实现向后兼容、不能再中间断行。
* 注释可以出现在文件的任何地方；注释包含可选的空格。以及后面的朱师傅、朱师傅后面的注释，知道行结束符为止。
* 0.0版的拒绝机器人访问标准并不支持Allow行。在这种情况下，机器人会比较保守，有些允许访问的URL它也不去获取。


